{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from podcast_ad_skipper.google_cloud import *\n",
    "from podcast_ad_skipper.data_preparation import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model, Sequential, layers, regularizers, optimizers, applications, models, callbacks\n",
    "from podcast_ad_skipper.params import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from podcast_ad_skipper.google_cloud import *\n",
    "\n",
    "from podcast_ad_skipper.model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Authenticated successfully with BigQuery! ✅\n"
     ]
    }
   ],
   "source": [
    "bq_client = auth_gc_bigquery()\n",
    "table_id = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.test10\"\n",
    "# columns = '[spectrogram, labels]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mQuery returned 5 results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rows = get_output_query_bigquery(bq_client, table_id, limit=5, columns=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_columns = get_bq_processed_data(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_count = 0\n",
    "spec_column = processed_columns[0]\n",
    "\n",
    "def check_for_nans(spec_column, nans_count):\n",
    "    for idx, array in enumerate(spec_column):\n",
    "        if np.isnan(array).any():\n",
    "                # print(f\"NaNs found in array at index {idx}\")\n",
    "            nans_count += 1\n",
    "    print(nans_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "check_for_nans(spec_column, nans_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128, 216, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.expand_dims(X, axis=-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, arr in enumerate(processed_columns[0]):\n",
    "#     X = processed_columns[0]\n",
    "#     if arr.shape != (128, 216):\n",
    "#         print(f\"Array at index {i} has shape {arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(processed_columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,  # This ensures similar class distribution in train/test splits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_baseline_model(input_shape=(128,216,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step - accuracy: 0.2500 - loss: 3.1921 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Sequential name=sequential_1, built=True>,\n",
       " <keras.src.callbacks.history.History at 0x178871480>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_gcs(model, bucket_name, model_name=\"latest_trained_model.h5\"):\n",
    "    \"\"\"\n",
    "    Save a TensorFlow model directly to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        model: TensorFlow model to save\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        model_name: Name of the model file\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bool, str) - (success status, message)\n",
    "    \"\"\"\n",
    "    # Initialize GCS client\n",
    "    gc_client = auth_gc_storage()\n",
    "\n",
    "    # Check if bucket exists\n",
    "    if not gc_client.bucket(bucket_name):\n",
    "        return False, f\"Bucket {bucket_name} not found\"\n",
    "\n",
    "    # Get bucket\n",
    "    bucket = gc_client.bucket(bucket_name)\n",
    "\n",
    "    # Create GCS URI for saving\n",
    "    gcs_uri = f\"gs://{bucket_name}/{model_name}\"\n",
    "\n",
    "    # Attempt to save model directly to GCS\n",
    "    save_status = models.save_model(\n",
    "        model,\n",
    "        gcs_uri,\n",
    "        overwrite=True,\n",
    "        include_optimizer=True,\n",
    "        save_format='h5'\n",
    "    )\n",
    "\n",
    "    # Check save status\n",
    "    if save_status is None:  # TensorFlow returns None on successful save\n",
    "        print(f\"Model saved successfully to {gcs_uri}\")\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to save model to GCS\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bucket = BUCKET_NAME_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Authenticated successfully with GCS! ✅\n",
      "Model saved successfully to gs://podcast-ad-skipper-model/latest_trained_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_to_gcs(model, model_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "def save_model_to_gcs(model, bucket_name, model_name=\"latest_trained_model.h5\",\n",
    "                     timeout=300, num_retries=3):\n",
    "    \"\"\"\n",
    "    Save a TensorFlow model to Google Cloud Storage with retry logic and configurable timeouts.\n",
    "\n",
    "    Args:\n",
    "        model: TensorFlow model to save\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        model_name: Name of the model file\n",
    "        timeout: Timeout in seconds for the upload operation (default: 300)\n",
    "        num_retries: Number of retry attempts (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bool, str) - (success status, message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize GCS client with increased timeout\n",
    "        storage_client = auth_gc_storage()\n",
    "\n",
    "        # Check if bucket exists\n",
    "        if not storage_client.lookup_bucket(bucket_name):\n",
    "            return False, f\"Bucket {bucket_name} not found\"\n",
    "\n",
    "        # Get bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create temporary file path\n",
    "            temp_model_path = os.path.join(temp_dir, model_name)\n",
    "\n",
    "            # Save model locally first\n",
    "            model.save(temp_model_path, save_format='h5')\n",
    "\n",
    "            # Configure retry logic\n",
    "            retry_config = retry.Retry(\n",
    "                initial=1.0,  # Initial delay in seconds\n",
    "                maximum=60.0,  # Maximum delay in seconds\n",
    "                multiplier=2.0,  # Multiplier for exponential backoff\n",
    "                deadline=timeout,  # Total timeout\n",
    "                predicate=retry.if_exception_type(\n",
    "                    TimeoutError,\n",
    "                    ConnectionError,\n",
    "                    ConnectionAbortedError\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Upload to GCS with retry logic\n",
    "            blob = bucket.blob(model_name)\n",
    "\n",
    "            # Set chunk size to 5MB for smaller models\n",
    "            blob.chunk_size = 5 * 1024 * 1024  # 5MB in bytes\n",
    "\n",
    "            for attempt in range(num_retries):\n",
    "                try:\n",
    "                    blob.upload_from_filename(\n",
    "                        temp_model_path,\n",
    "                        timeout=timeout,\n",
    "                        retry=retry_config\n",
    "                    )\n",
    "                    gcs_uri = f\"gs://{bucket_name}/{model_name}\"\n",
    "                    return True, f\"Model successfully saved to {gcs_uri}\"\n",
    "                except Exception as e:\n",
    "                    if attempt == num_retries - 1:  # Last attempt\n",
    "                        raise\n",
    "                    print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Error saving model to GCS: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Authenticated successfully with GCS! ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'Model successfully saved to gs://podcast-ad-skipper-model/latest_trained_model.h5')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_to_gcs(model, model_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 10 m and 9.5 s to save the model trained with 5 clips' worth of data\n",
    "# Will the model be heavier and hence take longer to save if so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_uri = f\"gs://{BUCKET_NAME_MODEL}/latest_trained_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Authenticated successfully with GCS! ✅\n",
      "\n",
      "❌ No model found in GCS bucket podcast-ad-skipper\n"
     ]
    }
   ],
   "source": [
    "model = download_model_from_gcs(gcs_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_from_gcs(bucket_name, model_name=\"latest_trained_model.h5\"):\n",
    "    \"\"\"\n",
    "    Download a TensorFlow model from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        model_name: Name of the model file\n",
    "\n",
    "    Returns:\n",
    "        model: TensorFlow model or None if download fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = auth_gc_storage()\n",
    "\n",
    "        # Check if bucket exists\n",
    "        if not storage_client.lookup_bucket(bucket_name):\n",
    "            return None, f\"Bucket {bucket_name} not found\"\n",
    "\n",
    "        # Get bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create temporary file path\n",
    "            temp_model_path = os.path.join(temp_dir, model_name)\n",
    "\n",
    "            # Download file from GCS\n",
    "            blob = bucket.blob(model_name)\n",
    "            blob.download_to_filename(temp_model_path)\n",
    "\n",
    "            # Load the model using TensorFlow\n",
    "            model = tf.keras.models.load_model(temp_model_path)\n",
    "\n",
    "            return model, f\"Model successfully downloaded from gs://{bucket_name}/{model_name}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Error downloading model from GCS: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Authenticated successfully with GCS! ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = download_model_from_gcs(BUCKET_NAME_MODEL, model_name=\"latest_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Sequential name=sequential_1, built=True>,\n",
       " 'Model successfully downloaded from gs://podcast-ad-skipper-model/latest_trained_model.h5')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 2 m and 41 s to download the model above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast-ad-skipper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
